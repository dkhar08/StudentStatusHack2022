{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91803235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train  = pd.read_csv(\"train_dataset_train.csv\")\n",
    "test = pd.read_csv(\"test_dataset_test.csv\")\n",
    "\n",
    "train.loc[train[\"Год_Поступления\"] == 2212, \"Год_Поступления\"] = 2012\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa265a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "614"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train[\"Статус\"] == -1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ce387a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_sex(df):\n",
    "    df.loc[df[\"Пол\"] == \"муж\", \"Пол\"] = \"Муж\"\n",
    "    df.loc[df[\"Пол\"] == \"жен\", \"Пол\"] = \"Жен\"\n",
    "\n",
    "    df.loc[df[\"Пол\"].isna(), \"Пол\"] = \"None\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac71d42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_language(df):\n",
    "    df[\"Изучаемый_Язык\"] = df[\"Изучаемый_Язык\"].fillna(\"None\")\n",
    "    \n",
    "    df.loc[df[\"Изучаемый_Язык\"].apply(lambda x: False if(x is None) else (\"Английский\" in x)), \"Изучаемый_Язык\"] = \"Английский\"\n",
    "    df.loc[df[\"Изучаемый_Язык\"].apply(lambda x: False if(x is None) else (\"Англиийский\" in x)), \"Изучаемый_Язык\"] = \"Английский\"\n",
    "    df.loc[df[\"Изучаемый_Язык\"].apply(lambda x: False if(x is None) else (\"Немецкий\" in x)), \"Изучаемый_Язык\"] = \"Немецкий\"\n",
    "    df.loc[df[\"Изучаемый_Язык\"].apply(lambda x: False if(x is None) else (\"Французский\" in x)), \"Изучаемый_Язык\"] = \"Французский\"\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "259e532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(x):\n",
    "    year = int(x.split('-')[0])\n",
    "    month = x.split('-')[1]\n",
    "    if(month[0]==\"0\"):\n",
    "        month = month[1]\n",
    "        \n",
    "    month =  int(month)\n",
    "    \n",
    "    return year*12 + month\n",
    "\n",
    "def parse_post(x):\n",
    "    return x*12 + 9\n",
    "    \n",
    "def fix_dates(df):\n",
    "    df[\"Дата_Рождения\"] = df[\"Дата_Рождения\"].apply(parse_date)\n",
    "    df[\"Год_Окончания_УЗ\"] = df[\"Год_Окончания_УЗ\"].fillna(-1)\n",
    "    df[\"gape\"] = df[\"Год_Поступления\"].values - df[\"Год_Окончания_УЗ\"].values\n",
    "    df[\"gape_age\"] = df[\"Год_Поступления\"].fillna(-1).apply(parse_post).values - df[\"Дата_Рождения\"].values\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7e0c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this set of features is aimed on seeking differences between peoples who relocated in order to study in university\n",
    "#and those who were not\n",
    "\n",
    "def find_lcsubstr(s1, s2):\n",
    "    m = [[0 for i in range(len(s2) + 1)] for j in range(len(s1) + 1)]  # Сгенерировать матрицу 0, чтобы облегчить последующие вычисления, на один столбец больше, чем длина строки\n",
    "    mmax = 0  # Длина самого длинного совпадения\n",
    "    p = 0  # Самое длинное совпадение соответствует последнему биту в s1\n",
    "    for i in range(len(s1)):\n",
    "        for j in range(len(s2)):\n",
    "            if s1[i] == s2[j]: # Если равно, добавить существующую общую подстроку\n",
    "                m[i + 1][j + 1] = m[i][j] + 1\n",
    "                if m[i + 1][j + 1] > mmax:\n",
    "                    mmax = m[i + 1][j + 1]\n",
    "                    p = i + 1\n",
    "    return s1[p - mmax:p], mmax\n",
    "\n",
    "def add_territorial_names_simmilarity(df):\n",
    "    names = [\"Страна_ПП\", \"Страна_Родители\", \"Город_ПП\", \"Регион_ПП\", \"Где_Находится_УЗ\", \"Уч_Заведение\"]\n",
    "\n",
    "    for i in range(len(names)-1):\n",
    "        for j in range(i + 1, len(names)):\n",
    "            v = []\n",
    "            for x, y in zip(df[names[i]], df[names[j]]):\n",
    "                if pd.notnull(x) and pd.notnull(y):\n",
    "                    #print(i, j, x,y)\n",
    "                    val = find_lcsubstr(x, y)[1]/min(len(x), len(y))\n",
    "                    v.append(val)\n",
    "                else:\n",
    "                    v.append(-1)\n",
    "\n",
    "        df[names[i] + '_' + names[j] + \"_diff\"] = v\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20dcf74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cityf(st):\n",
    "    r = ''\n",
    "    if pd.notnull(st):\n",
    "    \n",
    "        for w in st.split(' '):\n",
    "            if(len(w)==0):\n",
    "                continue\n",
    "            if(w[0].isupper()):\n",
    "                r += w\n",
    "        \n",
    "    if(len(r) == 0):\n",
    "        r = 'none'\n",
    "    return r\n",
    "\n",
    "#many regions have different ways of writing. So I unify them by word roots.\n",
    "def fix_territorial_names(df):\n",
    "    df[\"Страна_ПП\"] = df[\"Страна_ПП\"].fillna(\"None\")\n",
    "    \n",
    "    df[\"Страна_ПП\"] = df[\"Страна_ПП\"].apply(lambda x: x.lower())\n",
    "    \n",
    "    df.loc[train[\"Страна_ПП\"].apply(lambda x: (\"казах\" in x)), \"Страна_ПП\"] = \"казах\"\n",
    "    df.loc[train[\"Страна_ПП\"].apply(lambda x: (\"таджик\" in x)), \"Страна_ПП\"] = \"таджик\"\n",
    "    df.loc[train[\"Страна_ПП\"].apply(lambda x: (\"кырг\" in x)or(\"кирг\" in x)), \"Страна_ПП\"] = \"кырг\"\n",
    "    df.loc[train[\"Страна_ПП\"].apply(lambda x: (\"росс\" in x)), \"Страна_ПП\"] = \"росс\"\n",
    "    \n",
    "    df[\"Страна_Родители\"] = train[\"Страна_Родители\"].fillna(\"None\")\n",
    "    df[\"Страна_Родители\"] = train[\"Страна_Родители\"].apply(lambda x: x.lower())\n",
    "\n",
    "    df.loc[train[\"Страна_Родители\"].apply(lambda x: (\"казах\" in x)), \"Страна_Родители\"] = \"казах\"\n",
    "    df.loc[train[\"Страна_Родители\"].apply(lambda x: (\"таджик\" in x)), \"Страна_Родители\"] = \"таджик\"\n",
    "    df.loc[train[\"Страна_Родители\"].apply(lambda x: (\"кырг\" in x)or(\"кирг\" in x)), \"Страна_Родители\"] = \"кырг\"\n",
    "    df.loc[train[\"Страна_Родители\"].apply(lambda x: (\"росс\" in x)), \"Страна_Родители\"] = \"росс\"\n",
    "    df.loc[train[\"Страна_Родители\"].apply(lambda x: (\"кнр\" in x)), \"Страна_Родители\"] = \"китай\"\n",
    "    #city\n",
    "    df[\"Город_ПП\"] = df[\"Город_ПП\"].apply(cityf)\n",
    "    #---\n",
    "    df[\"Регион_ПП\"] = df[\"Регион_ПП\"].fillna(\"none\")\n",
    "    df[\"Регион_ПП\"] = df[\"Регион_ПП\"].apply(lambda a: a.lower())\n",
    "    \n",
    "    df[\"reg_pp\"] = df[\"Регион_ПП\"].copy()\n",
    "    df[\"reg_pp\"] = df[\"reg_pp\"].fillna(\"none\")\n",
    "\n",
    "    \n",
    "    ss = set([ \"алтайс\",\n",
    "     \"казах\",\n",
    "     \"тыва\",\n",
    "     \"кемер\", \n",
    "      \"саха\",\n",
    "      \"новосиб\",\n",
    "      \"павлодар\",\n",
    "     \"алтайс\",\n",
    "     \"краснояр\",\n",
    "      \"жалал\",\n",
    "     \"хатлон\",\n",
    "     \"казах\",\n",
    "     \"алмат\",\n",
    "     \"ляонин\",\n",
    "     \"хайнань\",\n",
    "     \"хэйлун\",\n",
    "     \"ямало\",\n",
    "      \"томск\",\n",
    "      \"цзилинь\",\n",
    "      \"иркут\",\n",
    "      \"бадах\",\n",
    "      \"иркут\",\n",
    "      \"ханты\"])\n",
    "\n",
    "\n",
    "    df.loc[df[\"Регион_ПП\"].apply(lambda x: (\"алтай\" in x)), \"Регион_ПП\"] = \"алтайс\"\n",
    "    df.loc[df[\"Регион_ПП\"].apply(lambda x: (\"казах\" in x)), \"Регион_ПП\"] = \"казах\"\n",
    "    df.loc[df[\"Регион_ПП\"].apply(lambda x: (\"тыва\" in x)), \"Регион_ПП\"] = \"тыва\"\n",
    "    df.loc[df[\"Регион_ПП\"].apply(lambda x: (\"кемер\" in x)), \"Регион_ПП\"] = \"кемер\"\n",
    "    df.loc[df[\"Регион_ПП\"].apply(lambda x: (\"саха\" in x)), \"Регион_ПП\"] = \"саха\"\n",
    "    df.loc[df[\"Регион_ПП\"].apply(lambda x: (\"новосиб\" in x)), \"Регион_ПП\"] = \"новосиб\"\n",
    "    df.loc[df[\"Регион_ПП\"].apply(lambda x: (\"павлодар\" in x)), \"Регион_ПП\"] = \"павлодар\"\n",
    "    df.loc[df[\"Регион_ПП\"].apply(lambda x: (\"алай\" in x)), \"Регион_ПП\"] = \"алтайс\"\n",
    "    df.loc[df[\"Регион_ПП\"].apply(lambda x: (\"краснояр\" in x)), \"Регион_ПП\"] = \"краснояр\"\n",
    "    df.loc[df[\"Регион_ПП\"].apply(lambda x: (\"жалал\" in x)), \"Регион_ПП\"] = \"жалал\"\n",
    "    df.loc[df[\"Регион_ПП\"].apply(lambda x: (\"хатлон\" in x)), \"Регион_ПП\"] = \"хатлон\"\n",
    "    df.loc[df[\"Регион_ПП\"].apply(lambda x: (\"вко\" in x)), \"Регион_ПП\"] = \"казах\"\n",
    "    df.loc[df[\"Регион_ПП\"].apply(lambda x: (\"алмат\" in x)), \"Регион_ПП\"] = \"алмат\"\n",
    "    df.loc[df[\"Регион_ПП\"].apply(lambda x: (\"ляонин\" in x)), \"Регион_ПП\"] = \"ляонин\"\n",
    "    df.loc[df[\"Регион_ПП\"].apply(lambda x: (\"хайнань\" in x)), \"Регион_ПП\"] = \"хайнань\"\n",
    "    df.loc[df[\"Регион_ПП\"].apply(lambda x: (\"хэйлун\" in x)), \"Регион_ПП\"] = \"хэйлун\"\n",
    "    df.loc[df[\"Регион_ПП\"].apply(lambda x: (\"ямало\" in x)), \"Регион_ПП\"] = \"ямало\"\n",
    "    df.loc[df[\"Регион_ПП\"].apply(lambda x: (\"томск\" in x)), \"Регион_ПП\"] = \"томск\"\n",
    "    df.loc[df[\"Регион_ПП\"].apply(lambda x: (\"цзилинь\" in x)), \"Регион_ПП\"] = \"цзилинь\"\n",
    "    df.loc[df[\"Регион_ПП\"].apply(lambda x: (\"иркут\" in x)), \"Регион_ПП\"] = \"иркут\"\n",
    "    df.loc[df[\"Регион_ПП\"].apply(lambda x: (\"бадах\" in x)), \"Регион_ПП\"] = \"бадах\"\n",
    "    df.loc[df[\"Регион_ПП\"].apply(lambda x: (\"иркут\" in x)), \"Регион_ПП\"] = \"иркут\"\n",
    "    df.loc[df[\"Регион_ПП\"].apply(lambda x: (\"ханты\" in x)), \"Регион_ПП\"] = \"ханты\"\n",
    "    df.loc[df[\"Регион_ПП\"].apply(lambda x: (\"омск\" in x)), \"Регион_ПП\"] = \"омск\"\n",
    "\n",
    "    df.loc[~df[\"Регион_ПП\"].isin(ss), \"Регион_ПП\"] = \"rare\"\n",
    "    #-----------\n",
    "    df[\"Где_Находится_УЗ\"] = df[\"Где_Находится_УЗ\"].fillna(\"none\")\n",
    "    df[\"Где_Находится_УЗ\"] = df[\"Где_Находится_УЗ\"].apply(lambda a:a.split(\",\")[-1]).apply(cityf)\n",
    "    #-----------\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11e84854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset contain certain amount of boolean variavles. I transform them into categorical with 3 values 0, 1, none==-1\n",
    "#I found out that model benefits from such NaN fillings strategy\n",
    "def fix_flags(df):\n",
    "    df[\"Пособие\"] = df[\"Пособие\"].fillna(-1)\n",
    "    \n",
    "    df[\"Общежитие\"] = df[\"Общежитие\"].fillna(-1)\n",
    "    \n",
    "    df[\"Наличие_Матери\"] = df[\"Наличие_Матери\"].fillna(-1)\n",
    "    \n",
    "    df[\"Наличие_Отца\"] = df[\"Наличие_Отца\"].fillna(-1)\n",
    "    \n",
    "    df[\"Иностранец\"] = df[\"Иностранец\"].fillna(-1)\n",
    "    \n",
    "    df[\"Опекунство\"] = df[\"Опекунство\"].fillna(-1)\n",
    "    \n",
    "    df[\"Село\"] = df[\"Село\"].fillna(-1)\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d700adc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "#function which exctracts bag of words features\n",
    "def transform_place(st, words2id):\n",
    "    x = np.zeros((len(words2id), ))\n",
    "    \n",
    "    if pd.notnull(st):\n",
    "        st = st.lower()\n",
    "        rez = ''.join([x if(x.isalpha() or x.isdigit()) else ' ' for x in st]).split()\n",
    "        for w in rez:\n",
    "            if w in words2id:\n",
    "                x[words2id[w]] += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f13425b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this variables will be signed as categorical for LGBM\n",
    "cat_feat = [\"Пол\", \"Основания\", \"Изучаемый_Язык\", \"Пособие\", \n",
    "            \"Страна_ПП\", \"Общежитие\", \"Наличие_Матери\", \"Наличие_Отца\", \"Страна_Родители\", \"Опекунство\", \n",
    "            \"Село\", \"Иностранец\", \"КодФакультета\", \"Город_ПП\", \"Регион_ПП\", \"Где_Находится_УЗ\", \"Уч_Заведение\", \"reg_pp\"]\n",
    "\n",
    "#drop answers from dataset))\n",
    "drop_feat = [ \"Статус\"] # 'ID', \"Код_группы\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a89d647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def merge_common(df, work_count, work_diff, work_diff_inv, dfid, dfidinv, uchzavdf):\n",
    "    #extract extra fetures from row with dataleak\n",
    "    df[\"f1\"] = df[\"Код_группы\"]//10000\n",
    "    df[\"f2\"] = df[\"Код_группы\"]//1000\n",
    "    df[\"f3\"] = df[\"Код_группы\"]//100\n",
    "    df[\"f4\"] = df[\"Код_группы\"]//10\n",
    "\n",
    "    df[\"l1\"] = df[\"Код_группы\"]%10000\n",
    "    df[\"l2\"] = df[\"Код_группы\"]%1000\n",
    "    df[\"l3\"] = df[\"Код_группы\"]%100\n",
    "    df[\"l4\"] = df[\"Код_группы\"]%10\n",
    "\n",
    "    df[\"df\"] = df[\"Код_группы\"]%2000\n",
    "    df[\"dl\"] = df[\"Код_группы\"]//2000\n",
    "    \n",
    "    count_name = \"Код_группы_count\"\n",
    "    name = \"Код_группы\"\n",
    "    \n",
    "    \n",
    "    #merge statistics which was calculated from train and test simultaneously\n",
    "\n",
    "    df = df.merge(work_count, on=name, how=\"left\") \n",
    "    df = df.merge(work_diff, on=name, how=\"left\") \n",
    "    df = df.merge(work_diff_inv, on=name, how=\"left\") \n",
    "    df = df.merge(dfid, on=\"ID\", how=\"left\") \n",
    "    df = df.merge(dfidinv, on=\"ID\", how=\"left\") \n",
    "\n",
    "\n",
    "    df = pd.concat([df, uchzavdf], axis=1)\n",
    "    \n",
    "\n",
    "    return df\n",
    "\n",
    "def common_calculated_features(train, test, cat_feat, drop_feat):\n",
    "    \n",
    "    for fixer in [fix_sex, fix_language, fix_dates, add_territorial_names_simmilarity, fix_territorial_names, fix_flags]:\n",
    "        train = fixer(train)\n",
    "        test = fixer(test)\n",
    "        \n",
    "    \n",
    "    #here I'm trying to squeze extra information from \"leaked\" rows\n",
    "\n",
    "    df = pd.concat([train, test], axis=0)\n",
    "    count_name = \"Код_группы_count\"\n",
    "    name = \"Код_группы\"\n",
    "    work_count = df[[name]].copy()\n",
    "    work_count[count_name] = 1\n",
    "    work_count = work_count.groupby(name).sum().reset_index()\n",
    "\n",
    "    sg = set(df[df[\"СрБаллАттестата\"]>5][\"Код_группы\"])&set(df[df[\"СрБаллАттестата\"]<=5][\"Код_группы\"])\n",
    "    a = np.zeros((len(train), ))\n",
    "    a[train[\"Код_группы\"].isin(sg)] = 1\n",
    "    train[\"Strange_disp\"] = a\n",
    "\n",
    "    a = np.zeros((len(test), ))\n",
    "    a[test[\"Код_группы\"].isin(sg)] = 1\n",
    "    test[\"Strange_disp\"] = a\n",
    "\n",
    "    a = np.array(list(set(df[\"Код_группы\"].values)))\n",
    "    a.sort()\n",
    "    b = np.hstack([np.array([-1]), a[1:] - a[:-1]])\n",
    "    work_diff = pd.DataFrame.from_dict({\"Код_группы\":a, \"diff_group\":b})\n",
    "\n",
    "    a = np.array(list(set(df[\"Код_группы\"].values)))\n",
    "    a.sort()\n",
    "    b = np.hstack([a[1:] - a[:-1], np.array([-1])])\n",
    "    work_diff_inv = pd.DataFrame.from_dict({\"Код_группы\":a, \"diff_group_inv\":b})\n",
    "\n",
    "    a = np.array(list(set(df[\"ID\"].values)))\n",
    "    a.sort()\n",
    "    b = np.hstack([np.array([-1]), a[1:] - a[:-1]])\n",
    "    dfid = pd.DataFrame.from_dict({\"ID\":a, \"diff_ID\":b})\n",
    "\n",
    "    a = np.array(list(set(df[\"ID\"].values)))\n",
    "    a.sort()\n",
    "    b = np.hstack([a[1:] - a[:-1], np.array([-1])])\n",
    "    dfidinv = pd.DataFrame.from_dict({\"ID\":a, \"diff_ID_ind\":b})\n",
    "    \n",
    "    #extract bow features from previous place of study\n",
    "    ct = Counter()\n",
    "    for st in train[~train[\"Уч_Заведение\"].isna()][\"Уч_Заведение\"]:\n",
    "        st = st.lower()\n",
    "        rez = ''.join([x if(x.isalpha()or x.isdigit()) else ' ' for x in st])\n",
    "        ct += Counter( rez.split() ) \n",
    "\n",
    "    filtered_words = set([word for word, freq in ct.most_common() if freq>20])\n",
    "    words2id  = {word:i for i, word in enumerate(filtered_words)}\n",
    "    id2word = {v:k for k,v in words2id.items()}\n",
    "    \n",
    "    A = np.vstack(train[\"Уч_Заведение\"].apply(lambda x: transform_place(x, words2id)).tolist())\n",
    "    uchzavdf = pd.DataFrame(A, columns = [id2word[i]+\"_учзав\" for i in range(len(filtered_words))])\n",
    "\n",
    "    A = np.vstack(test[\"Уч_Заведение\"].apply(lambda x: transform_place(x, words2id)).tolist())\n",
    "    uchzavdf_test = pd.DataFrame(A, columns = [id2word[i]+\"_учзав\" for i in range(len(filtered_words))])\n",
    "    \n",
    "    #mark categorical features\n",
    "    feature_columns = list(train.columns.values)\n",
    "    for x in drop_feat:\n",
    "        feature_columns.remove(x)\n",
    "    \n",
    "    X_TRAIN = train[feature_columns].copy()\n",
    "    X_TEST = test[feature_columns].copy()\n",
    "    y = train[\"Статус\"].values\n",
    "    \n",
    "    for x in cat_feat:\n",
    "        X_TRAIN[x] = X_TRAIN[x].astype(\"category\")\n",
    "        X_TEST[x] = X_TEST[x].astype(\"category\")\n",
    "\n",
    "        \n",
    "\n",
    "    X_TRAIN = merge_common(X_TRAIN, work_count, work_diff, work_diff_inv, dfid, dfidinv, uchzavdf)\n",
    "    X_TEST = merge_common(X_TEST, work_count, work_diff, work_diff_inv, dfid, dfidinv, uchzavdf_test)\n",
    "    \n",
    "    \n",
    "    X_TRAIN[\"Status\"] = y\n",
    "    X_TRAIN = X_TRAIN.sort_values([\"Код_группы\", \"ID\"])\n",
    "    y = X_TRAIN[\"Status\"].values\n",
    "    X_TRAIN = X_TRAIN.drop(columns=[\"Status\"])\n",
    "    X_TRAIN = X_TRAIN.reset_index()\n",
    "    X_TRAIN = X_TRAIN.drop(columns=[\"index\"])\n",
    "    \n",
    "    \n",
    "    return X_TRAIN, y, X_TEST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9003fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#there should have been several layer of stacking.\n",
    "#but I find out that it is not benefical for final solution\n",
    "#So that functions may look little bit sophisticated\n",
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)\n",
    "\n",
    "def fit_lgbms(params, cat_feat, X, y):\n",
    "    lgbms = []\n",
    "    seeds = [154, 2277, 42, 5, 8713]\n",
    "    for seed in seeds:\n",
    "        \n",
    "        lgbm = LGBMClassifier(is_unbalance=True, **params, random_state=seed)\n",
    "        \n",
    "        lgbm.fit(X, y, categorical_feature=cat_feat)\n",
    "        \n",
    "        lgbms.append(lgbm)\n",
    "        \n",
    "    return lgbms\n",
    "\n",
    "def fit_lgbms_full(params, cat_feat, X, y):\n",
    "    lgbms = []\n",
    "    seeds = [154, 2277, 42, 5, 8713]\n",
    "    for seed in seeds:\n",
    "        \n",
    "        lgbm = LGBMClassifier(class_weight=\"balanced\", **params, random_state=seed)\n",
    "        \n",
    "        lgbm.fit(X, y, categorical_feature=cat_feat)\n",
    "        \n",
    "        lgbms.append(lgbm)\n",
    "        \n",
    "    return lgbms\n",
    "\n",
    "\n",
    "def prdict_lgbms(lgbms, X):\n",
    "    \n",
    "    ys = []\n",
    "    \n",
    "    for lgbm in lgbms:\n",
    "        \n",
    "        y = lgbm.predict(X)\n",
    "        ys.append(y)\n",
    "        \n",
    "    yf = []\n",
    "    for i in range(len(X)):\n",
    "        l = [ys[j][i] for j in range(len(lgbms))]\n",
    "        yf.append(most_common(l))\n",
    "           \n",
    "    return yf\n",
    "\n",
    "\n",
    "def prdict_lgbms_proba(lgbms, X):\n",
    "    \n",
    "    ys = []\n",
    "    \n",
    "    for lgbm in lgbms:\n",
    "        \n",
    "        y = lgbm.predict_proba(X)[:,1].reshape(1, -1)\n",
    "        ys.append(y)\n",
    "               \n",
    "    return np.vstack(ys).mean(axis=0)\n",
    "\n",
    "def prdict_lgbms_proba_full(lgbms, X):\n",
    "    \n",
    "    ys = []\n",
    "    \n",
    "    for lgbm in lgbms:\n",
    "        \n",
    "        y = lgbm.predict_proba(X) #[:,1].reshape(1, -1)\n",
    "        ys.append(y)\n",
    "               \n",
    "    return np.stack(ys).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afb4dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sm_bagged_fit_predict(X_TRAIN, y, X_TEST, sm_params, n_folds, seeds):\n",
    "    ovo_test_list = []\n",
    "    sm_train_list = []\n",
    "       \n",
    "    features_sm_test = []\n",
    "        \n",
    "    for seed in seeds:\n",
    "        \n",
    "        features_ovo = []\n",
    "\n",
    "        preds = []\n",
    "    \n",
    "\n",
    "        for ti, di in StratifiedKFold(n_splits=n_folds, shuffle=True,  random_state=seed).split(X_TRAIN, y):\n",
    "\n",
    "            Xt = X_TRAIN.iloc[ti]\n",
    "            yt = y[ti]\n",
    "\n",
    "\n",
    "            Xd = X_TRAIN.iloc[di]\n",
    "            yd = y[di]\n",
    "\n",
    "            lgbms = fit_lgbms_full(sm_params, cat_feat, Xt, yt)\n",
    "            ypred = prdict_lgbms_proba_full(lgbms, Xd)\n",
    "            features_sm_test.append(prdict_lgbms_proba_full(lgbms, X_TEST))\n",
    "\n",
    "            preds.append((di, ypred))\n",
    "\n",
    "                    \n",
    "        \n",
    "        yprom = np.zeros((len(X_TRAIN), 3))\n",
    "\n",
    "        for i in range(n_folds):\n",
    "            yprom[preds[i][0]] = preds[i][1]\n",
    "\n",
    "        sm_train_list.append(yprom)\n",
    "    \n",
    "    return np.stack(features_sm_test).mean(axis=0), np.stack(sm_train_list).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2cb6acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optuna selected hyperparametrs\n",
    "sm_params = {'n_estimators': 500,\n",
    " 'learning_rate': 0.01587687545192069,\n",
    " 'num_leaves': 260,\n",
    " 'min_child_samples': 65,\n",
    " 'min_split_gain': 0.00044563404841053283,\n",
    " 'subsample': 0.9,\n",
    " 'subsample_freq': 1,\n",
    " 'colsample_bytree': 0.4,\n",
    " 'max_cat_threshold': 8,\n",
    " 'cat_l2': 94.8926686487861,\n",
    " 'min_sum_hessian_in_leaf': 0.090931378259608,\n",
    " 'max_depth': -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c0e878",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN, y, X_TEST = common_calculated_features(train, test, cat_feat, drop_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f8821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Ct = sm_bagged_fit_predict(X_TRAIN, y, X_TEST, sm_params, 5, [333, 444, 555, 777, 888]) #[333, 444, 555, 777, 888]\n",
    "Cd = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23964c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = np.array([-1, 3, 4])[np.argmax(Cd, axis=1)] #Cd\n",
    "\n",
    "subm = pd.read_csv(\"sample_submission.csv\")\n",
    "subm[\"Статус\"] = ypred\n",
    "subm.to_csv(\"rez.csv\", encoding='utf-8', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe9531a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
